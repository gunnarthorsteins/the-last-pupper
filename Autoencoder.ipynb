{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dying-invasion",
   "metadata": {},
   "source": [
    "# Training of a Convolutional Autoencoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sharing-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from utils.image_scrape import *\n",
    "from utils.image_formatting import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-confidence",
   "metadata": {},
   "source": [
    "We load the data, making use of our utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handy-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pablo-picasso\n",
      "william-turner\n",
      "pierre-auguste-renoir\n",
      "vincent-van-gogh\n",
      "paul-cezanne\n",
      "claude-monet\n",
      "edouard-manet\n",
      "jacques-louis-david\n",
      "gustave-courbet\n",
      "eugene-delacroix\n",
      "X_train shape: (2400, 256, 256, 3)\n",
      "X_val shape: (600, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "artists = list(PAINTER_DICT.keys())\n",
    "X, y = preprocess_images(artists=artists, n_imgs=300, normalize=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)   # We don't need labels to train the autoencoder because it is unsupervised\n",
    "X_train = apply_dropout(X_train, dropout_rate=0)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-forward",
   "metadata": {},
   "source": [
    "Setting up the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "growing-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_1 (Conv2D)            (None, 256, 256, 200)     15200     \n",
      "_________________________________________________________________\n",
      "MaxPool_1 (MaxPooling2D)     (None, 128, 128, 200)     0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 128, 128, 200)     1000200   \n",
      "_________________________________________________________________\n",
      "MaxPool_2 (MaxPooling2D)     (None, 64, 64, 200)       0         \n",
      "_________________________________________________________________\n",
      "UpSample_1 (UpSampling2D)    (None, 128, 128, 200)     0         \n",
      "_________________________________________________________________\n",
      "DeConv2D_1 (Conv2DTranspose) (None, 128, 128, 200)     1000200   \n",
      "_________________________________________________________________\n",
      "UpSample_2 (UpSampling2D)    (None, 256, 256, 200)     0         \n",
      "_________________________________________________________________\n",
      "DeConv2D_2 (Conv2DTranspose) (None, 256, 256, 200)     1000200   \n",
      "_________________________________________________________________\n",
      "DeConv2D_Reconstruction (Con (None, 256, 256, 3)       603       \n",
      "=================================================================\n",
      "Total params: 3,016,403\n",
      "Trainable params: 3,016,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train[0].shape[0], X_train[0].shape[1], X_train[0].shape[2]) # this should NOT take into account batches. Keras adds batch dimensions\n",
    "activation = \"relu\"\n",
    "regularizer = tf.keras.regularizers.L1L2(l1=0, l2=0)\n",
    "n_filters = 200\n",
    "\n",
    "def create_autoencoder():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(Conv2D(filters=n_filters, kernel_size=(5, 5), strides=(1,1),\n",
    "                         activation=activation, input_shape=input_shape,\n",
    "                         padding=\"same\", name='Conv2D_1', kernel_regularizer=regularizer))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "                               padding='valid', name='MaxPool_1')),\n",
    "        model.add(Conv2D(filters=n_filters, kernel_size=(5, 5), strides=(1,1),\n",
    "                         activation=activation, padding=\"same\", name='Conv2D_2', kernel_regularizer=regularizer))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "                               padding='valid', name=\"MaxPool_2\"))\n",
    "        model.add(UpSampling2D(interpolation='bilinear', name='UpSample_1'))\n",
    "        model.add(Conv2DTranspose(filters=n_filters, kernel_size=(5, 5),\n",
    "                                  strides=(1,1), activation=activation, padding=\"same\", name='DeConv2D_1'))\n",
    "        model.add(UpSampling2D(interpolation='bilinear', name='UpSample_2'))\n",
    "        model.add(Conv2DTranspose(filters=n_filters, kernel_size=(5, 5),\n",
    "                                  strides=(1,1), activation=activation, padding=\"same\", name='DeConv2D_2'))\n",
    "        model.add(Conv2DTranspose(filters=3, kernel_size=(1, 1), strides=(1,1),\n",
    "                                  activation=activation,\n",
    "                                  name='DeConv2D_Reconstruction'))    # reconstruct image\"\n",
    "        return model\n",
    "\n",
    "autoencoder = create_autoencoder()\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss=losses.MeanSquaredError(),\n",
    "                    metrics=['KLDivergence'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "following-compensation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 495s 7s/step - loss: 0.3687 - kullback_leibler_divergence: 1.3577 - val_loss: 0.0055 - val_kullback_leibler_divergence: 0.0737\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 482s 6s/step - loss: 0.0043 - kullback_leibler_divergence: 0.0344 - val_loss: 0.0038 - val_kullback_leibler_divergence: 0.0762\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.0040 - kullback_leibler_divergence: 0.0335 - val_loss: 0.0035 - val_kullback_leibler_divergence: 0.0636\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.0029 - kullback_leibler_divergence: 0.0284 - val_loss: 0.0026 - val_kullback_leibler_divergence: 0.0154\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 483s 6s/step - loss: 0.0031 - kullback_leibler_divergence: 0.0291 - val_loss: 0.0026 - val_kullback_leibler_divergence: 0.0277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fddf6b07c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=5,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-sending",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collective-vatican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/anaconda3/envs/envTF22/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/autoencoder_artists_depth_relu_2021-04-23-21-10/assets\n",
      "Model saved\n",
      "2021-04-23-21-10\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "autoencoder_path = f'models/autoencoder_artists_depth_{activation}_{timestamp}'\n",
    "autoencoder.save(autoencoder_path)\n",
    "print('Model saved')\n",
    "print(timestamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
